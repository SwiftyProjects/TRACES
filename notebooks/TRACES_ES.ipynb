{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea9f700-1b97-4f4c-b84e-2247398677e7",
   "metadata": {},
   "source": [
    "TRACES (Time-series Relationship Analysis with Comprehensive Evaluation Suite)\n",
    "==========================================================================\n",
    "\n",
    "A Hierarchical Multi-Method Time Series Correlation Analyzer\n",
    "\n",
    "Overview:\n",
    "---------\n",
    "TRACES is a comprehensive framework for analyzing relationships between time series data using multiple correlation methods. It automatically determines the most appropriate correlation method(s) for each pair of series and provides detailed visualizations and analysis.\n",
    "\n",
    "Key Features:\n",
    "------------\n",
    "- Multi-method correlation analysis (Pearson, Spearman, Kendall, CCF)\n",
    "- Automatic relationship type classification (linear, non-linear, lagged, complex)\n",
    "- Hierarchical data structure support (parent-child relationships)\n",
    "- Advanced visualization suite\n",
    "- Comprehensive statistical testing\n",
    "- Flexible time series handling\n",
    "\n",
    "Input Requirements:\n",
    "------------------\n",
    "- Excel file (.xlsx)\n",
    "- First row: Column headers with series names\n",
    "- First column: Time intervals (1 to n)\n",
    "- Additional columns: Time series data\n",
    "- Supports variable numbers of series and intervals\n",
    "\n",
    "Output Components:\n",
    "-----------------\n",
    "1. Correlation Analysis:\n",
    "   - Basic correlations (Pearson, Spearman, Kendall)\n",
    "   - Cross-correlation function (CCF)\n",
    "   - Time-delayed correlations\n",
    "   - Rolling correlations\n",
    "\n",
    "2. Visualization Suite:\n",
    "   - Time series comparisons\n",
    "   - Correlation method comparisons\n",
    "   - Cross-correlation plots\n",
    "   - Rolling correlation trends\n",
    "\n",
    "3. Results Classification:\n",
    "   - Relationship type identification\n",
    "   - Best method recommendations\n",
    "   - Confidence metrics\n",
    "   - Summary statistics\n",
    "\n",
    "Usage Notes:\n",
    "-----------\n",
    "- Handles parent-child relationships in data hierarchy\n",
    "- Automatically excludes invalid comparisons\n",
    "- Provides both individual pair and full dataset analysis\n",
    "- Supports various data types and scales\n",
    "\n",
    "Structure:\n",
    "---------\n",
    "The notebook is organized into 6 sequential steps:\n",
    "1. Environment setup and data loading\n",
    "2. Core correlation functions\n",
    "3. Advanced correlation methods\n",
    "4. Analysis framework\n",
    "5. Visualization functions\n",
    "6. Results processing\n",
    "\n",
    "Dependencies: pandas, numpy, scipy, matplotlib, seaborn\n",
    "Performance: Optimized for datasets with up to 1000+ pair comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a16199-d9f7-4dae-85bf-10feb9f39377",
   "metadata": {},
   "source": [
    "Component Flexibility Guide:\n",
    "-------------------------\n",
    "GREEN ZONE (Highly Customizable):\n",
    "- Configuration parameters (rolling window size, max lag, significance levels)\n",
    "- Visualization settings (plot sizes, colors, layout)\n",
    "- Results sorting and filtering criteria\n",
    "- Parent-child relationship definitions\n",
    "- Output format and summary statistics\n",
    "\n",
    "YELLOW ZONE (Modify with Caution):\n",
    "- Correlation method thresholds\n",
    "- Relationship type classification rules\n",
    "- Confidence score calculations\n",
    "- CCF normalization approach\n",
    "- Time delay ranges\n",
    "\n",
    "RED ZONE (Core Framework - Handle with Care):\n",
    "- Base correlation calculations\n",
    "- Method comparison logic\n",
    "- Parent-child exclusion mechanism\n",
    "- Statistical testing fundamentals\n",
    "- Core data structure handling\n",
    "\n",
    "Critical Dependencies:\n",
    "--------------------\n",
    "- Parent-child mappings must be explicitly defined\n",
    "- Time series must be continuous and ordered\n",
    "- Column names must be consistent throughout\n",
    "- Minimum of 3 data points per series\n",
    "- Input data must be numeric (except time labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0766cac-f251-4862-a55b-a1154c25f02b",
   "metadata": {},
   "source": [
    "Operational Flow and Cell Dependencies:\n",
    "------------------------------------\n",
    "\n",
    "INITIAL BUILD/MODIFICATION (All Cells Required):\n",
    "Run in strict sequence 1-6 when:\n",
    "- First time setup\n",
    "- Modifying any function\n",
    "- Changing core parameters\n",
    "- Adding new methods\n",
    "- Updating visualizations\n",
    "\n",
    "STANDARD ANALYSIS (After Initial Build):\n",
    "Required Minimum Flow:\n",
    "1. Cell 1 (Setup & Environment) - ALWAYS REQUIRED\n",
    "6. Cell 6 (Full Analysis) - PRIMARY EXECUTION\n",
    "\n",
    "TARGETED ANALYSIS OPTIONS:\n",
    "For specific pairs/visualization:\n",
    "1. Cell 1 (Setup) → 5. Cell 5 (Visualization)\n",
    "For method comparison only:\n",
    "1. Cell 1 (Setup) → 4. Cell 4 (Method Comparison)\n",
    "\n",
    "USE CASE SCENARIOS:\n",
    "1. Full Dataset Analysis:\n",
    "  - Run Cell 1, then Cell 6\n",
    "  - Returns complete analysis of all pairs\n",
    "\n",
    "2. Single Pair Deep Dive:\n",
    "  - Run Cell 1\n",
    "  - Run Cell 5\n",
    "  - Provides detailed visualization suite\n",
    "\n",
    "3. Method Testing:\n",
    "  - Cells 1-4 required\n",
    "  - Useful for methodology validation\n",
    "\n",
    "**NOTE: Once built, the notebook maintains state until kernel reset. Cell 6 contains all necessary function calls from previous cells.**\n",
    "\n",
    "Memory Management:\n",
    "- Clear output between runs for large datasets\n",
    "- Restart kernel if changing parent-child relationships\n",
    "- Consider batch processing for very large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee16a72-f209-46e1-96c8-1c84bfdc1c4f",
   "metadata": {},
   "source": [
    "# Step 1 of 6: Setup and Environment Configuration\n",
    "- Dependencies: None\n",
    "- Outputs: Configured environment with required libraries and global parameters\n",
    "\n",
    "Notes:\n",
    "- Handles data structure setup\n",
    "- Defines parent-child relationships\n",
    "- Sets global parameters for analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b28861-7979-43b4-b143-e02346455dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 of 6: Setup and Environment Configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Example of parent-child relationships structure\n",
    "PARENT_CHILD_MAPPING = {\n",
    "    \"Parent_Category\": [\n",
    "        \"Child_Category_1\",\n",
    "        \"Child_Category_2\",\n",
    "        \"Child_Category_3\",\n",
    "        \"Child_Category_4\",\n",
    "        \"Child_Category_5\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Analysis configuration with default values\n",
    "CONFIG = {\n",
    "    'rolling_window': 12,     # Default window size for rolling correlations\n",
    "    'max_lag': 10,           # Default maximum lag to consider\n",
    "    'significance_level': 0.05,  # Default significance level for statistical testing\n",
    "    'min_correlation': 0.3    # Default minimum correlation threshold\n",
    "}\n",
    "\n",
    "def load_and_prepare_data(file_path: str) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Load time series data from Excel file and prepare valid comparison pairs.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to Excel file containing time series data.\n",
    "            Expected format:\n",
    "            - First column: Time intervals\n",
    "            - Other columns: Time series data\n",
    "            - First row: Column headers (series names)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, List[str]]: \n",
    "            - Processed DataFrame with time series data\n",
    "            - List of valid comparison pairs (excluding parent-child relationships)\n",
    "    \n",
    "    Example:\n",
    "        >>> df, valid_pairs = load_and_prepare_data(\"time_series_data.xlsx\")\n",
    "        >>> print(f\"Loaded {len(df)} time points and {len(valid_pairs)} comparison pairs\")\n",
    "    \"\"\"\n",
    "    # Read data\n",
    "    df = pd.read_excel(file_path, header=0)\n",
    "    \n",
    "    # Create list of valid comparison pairs (excluding parent-child)\n",
    "    all_columns = [col for col in df.columns if col != 'Interval']\n",
    "    valid_pairs = []\n",
    "    \n",
    "    for i, col1 in enumerate(all_columns):\n",
    "        for col2 in all_columns[i+1:]:\n",
    "            # Skip parent-child comparisons\n",
    "            is_parent_child = False\n",
    "            for parent, children in PARENT_CHILD_MAPPING.items():\n",
    "                if (col1 == parent and col2 in children) or \\\n",
    "                   (col2 == parent and col1 in children):\n",
    "                    is_parent_child = True\n",
    "                    break\n",
    "            \n",
    "            if not is_parent_child:\n",
    "                valid_pairs.append((col1, col2))\n",
    "    \n",
    "    return df, valid_pairs\n",
    "\n",
    "def normalize_series(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Normalize a series to zero mean and unit variance.\n",
    "    \n",
    "    Args:\n",
    "        series (pd.Series): Input time series data\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Normalized time series with mean=0 and std=1\n",
    "    \n",
    "    Example:\n",
    "        >>> normalized_data = normalize_series(df['series_name'])\n",
    "    \"\"\"\n",
    "    return (series - series.mean()) / series.std()\n",
    "\n",
    "# Example usage (commented out for library import)\n",
    "\"\"\"\n",
    "try:\n",
    "    file_path = 'example_data.xlsx'\n",
    "    df, valid_pairs = load_and_prepare_data(file_path)\n",
    "    print(f\"Successfully loaded data with {len(df)} rows and {len(df.columns)} columns\")\n",
    "    print(f\"Generated {len(valid_pairs)} valid comparison pairs\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed97a9d-339d-4476-abfb-c3f500988670",
   "metadata": {},
   "source": [
    "# Step 2 of 6: Core Correlation Functions\n",
    "\n",
    "- Dependencies: Cell 1 (imports, data loading, and configurations)\n",
    "- Outputs: Basic correlation calculations and initial comparison framework\n",
    "\n",
    "Notes:\n",
    "- Implements Pearson, Spearman, and Kendall's Tau correlations\n",
    "- Includes significance testing\n",
    "- Prepares results in comparable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b1f029-aafd-4975-9185-18c3c71c45aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 of 6: Core Correlation Functions\n",
    "def calculate_basic_correlations(series1: pd.Series, series2: pd.Series) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate Pearson, Spearman, and Kendall correlations between two time series.\n",
    "    \n",
    "    Args:\n",
    "        series1 (pd.Series): First time series\n",
    "        series2 (pd.Series): Second time series\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Dictionary containing correlation results with format:\n",
    "            {\n",
    "                'method_name': {\n",
    "                    'correlation': float,  # Correlation coefficient\n",
    "                    'p_value': float,      # Statistical significance\n",
    "                    'significant': bool     # True if p < significance_level\n",
    "                }\n",
    "            }\n",
    "            \n",
    "    Example:\n",
    "        >>> results = calculate_basic_correlations(df['series1'], df['series2'])\n",
    "        >>> print(f\"Pearson correlation: {results['pearson']['correlation']:.3f}\")\n",
    "    \"\"\"\n",
    "    # Normalize series\n",
    "    s1_norm = normalize_series(series1)\n",
    "    s2_norm = normalize_series(series2)\n",
    "    \n",
    "    # Calculate correlations and p-values\n",
    "    pearson_corr, pearson_p = pearsonr(s1_norm, s2_norm)\n",
    "    spearman_corr, spearman_p = spearmanr(s1_norm, s2_norm)\n",
    "    kendall_corr, kendall_p = kendalltau(s1_norm, s2_norm)\n",
    "    \n",
    "    return {\n",
    "        'pearson': {\n",
    "            'correlation': pearson_corr,\n",
    "            'p_value': pearson_p,\n",
    "            'significant': pearson_p < CONFIG['significance_level']\n",
    "        },\n",
    "        'spearman': {\n",
    "            'correlation': spearman_corr,\n",
    "            'p_value': spearman_p,\n",
    "            'significant': spearman_p < CONFIG['significance_level']\n",
    "        },\n",
    "        'kendall': {\n",
    "            'correlation': kendall_corr,\n",
    "            'p_value': kendall_p,\n",
    "            'significant': kendall_p < CONFIG['significance_level']\n",
    "        }\n",
    "    }\n",
    "\n",
    "def calculate_rolling_correlation(series1: pd.Series, series2: pd.Series) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate rolling correlations using configured window size.\n",
    "    \n",
    "    Args:\n",
    "        series1 (pd.Series): First time series\n",
    "        series2 (pd.Series): Second time series\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Dictionary containing rolling correlation results:\n",
    "            {\n",
    "                'rolling_correlation': {\n",
    "                    'values': pd.Series,  # Rolling correlation values\n",
    "                    'mean': float,        # Mean correlation\n",
    "                    'std': float,         # Standard deviation\n",
    "                    'max': float,         # Maximum correlation\n",
    "                    'min': float          # Minimum correlation\n",
    "                }\n",
    "            }\n",
    "            \n",
    "    Example:\n",
    "        >>> results = calculate_rolling_correlation(df['series1'], df['series2'])\n",
    "        >>> print(f\"Mean rolling correlation: {results['rolling_correlation']['mean']:.3f}\")\n",
    "    \"\"\"\n",
    "    # Normalize series\n",
    "    s1_norm = normalize_series(series1)\n",
    "    s2_norm = normalize_series(series2)\n",
    "    \n",
    "    # Calculate rolling correlations\n",
    "    rolling_pearson = pd.Series(s1_norm).rolling(window=CONFIG['rolling_window'])\\\n",
    "        .corr(pd.Series(s2_norm))\n",
    "    \n",
    "    return {\n",
    "        'rolling_correlation': {\n",
    "            'values': rolling_pearson,\n",
    "            'mean': rolling_pearson.mean(),\n",
    "            'std': rolling_pearson.std(),\n",
    "            'max': rolling_pearson.max(),\n",
    "            'min': rolling_pearson.min()\n",
    "        }\n",
    "    }\n",
    "\n",
    "def identify_best_correlation_method(results: Dict) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Identify the correlation method showing strongest relationship.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Dictionary of correlation results from calculate_basic_correlations()\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[str, float]: (best_method_name, correlation_value)\n",
    "        \n",
    "    Example:\n",
    "        >>> basic_results = calculate_basic_correlations(df['series1'], df['series2'])\n",
    "        >>> method, value = identify_best_correlation_method(basic_results)\n",
    "        >>> print(f\"Best method: {method} with correlation {value:.3f}\")\n",
    "    \"\"\"\n",
    "    methods = {\n",
    "        'pearson': abs(results['pearson']['correlation']),\n",
    "        'spearman': abs(results['spearman']['correlation']),\n",
    "        'kendall': abs(results['kendall']['correlation'])\n",
    "    }\n",
    "    \n",
    "    best_method = max(methods.items(), key=lambda x: x[1])\n",
    "    return best_method[0], best_method[1]\n",
    "\n",
    "# Example usage (commented out for library import)\n",
    "\"\"\"\n",
    "def example_correlation_analysis():\n",
    "    # Create sample data\n",
    "    dates = pd.date_range(start='2023-01-01', periods=100, freq='D')\n",
    "    series1 = pd.Series(np.random.randn(100).cumsum(), index=dates)\n",
    "    series2 = pd.Series(np.random.randn(100).cumsum(), index=dates)\n",
    "    \n",
    "    # Calculate basic correlations\n",
    "    basic_results = calculate_basic_correlations(series1, series2)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nBasic Correlation Results:\")\n",
    "    for method, results in basic_results.items():\n",
    "        print(f\"\\n{method.capitalize()}:\")\n",
    "        print(f\"Correlation: {results['correlation']:.4f}\")\n",
    "        print(f\"P-value: {results['p_value']:.4f}\")\n",
    "        print(f\"Significant: {results['significant']}\")\n",
    "    \n",
    "    # Calculate rolling correlations\n",
    "    rolling_results = calculate_rolling_correlation(series1, series2)\n",
    "    print(\"\\nRolling Correlation Summary:\")\n",
    "    print(f\"Mean: {rolling_results['rolling_correlation']['mean']:.4f}\")\n",
    "    print(f\"Std: {rolling_results['rolling_correlation']['std']:.4f}\")\n",
    "    \n",
    "    # Identify best method\n",
    "    best_method, best_value = identify_best_correlation_method(basic_results)\n",
    "    print(f\"\\nBest correlation method: {best_method} (|r| = {best_value:.4f})\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f7ff5-cdeb-46f5-9818-b710d70e9669",
   "metadata": {},
   "source": [
    "# Step 3 of 6: Advanced Correlation Methods and CCF Analysis\n",
    "- Dependencies: Cells 1-2 (imports, data loading, basic correlations)\n",
    "- Outputs: Advanced correlation measures including CCF, time-delayed analysis\n",
    "\n",
    "Notes:\n",
    "- Enhances existing CCF analysis\n",
    "- Adds time-delayed correlations\n",
    "- Includes comprehensive statistical testing\n",
    "- Prepares for method comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5833c6-54c4-4aef-82b5-55135900d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 of 6: Advanced Correlation Methods and CCF Analysis\n",
    "def calculate_ccf(series1: pd.Series, series2: pd.Series, max_lag: int = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate Cross Correlation Function (CCF) between two time series.\n",
    "    \n",
    "    Args:\n",
    "        series1 (pd.Series): First time series\n",
    "        series2 (pd.Series): Second time series\n",
    "        max_lag (int, optional): Maximum lag to consider. Defaults to CONFIG['max_lag']\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Dictionary containing CCF analysis results:\n",
    "            {\n",
    "                'ccf': {\n",
    "                    'correlation': float,      # Maximum correlation value\n",
    "                    'optimal_lag': int,        # Lag at maximum correlation\n",
    "                    'zero_lag_correlation': float,  # Correlation at lag=0\n",
    "                    'all_correlations': array, # All correlation values\n",
    "                    'all_lags': array,         # All lag values\n",
    "                    'lag_strength_ratio': float # Ratio of max to zero-lag correlation\n",
    "                }\n",
    "            }\n",
    "            \n",
    "    Example:\n",
    "        >>> results = calculate_ccf(df['series1'], df['series2'])\n",
    "        >>> print(f\"Optimal lag: {results['ccf']['optimal_lag']}\")\n",
    "    \"\"\"\n",
    "    [Rest of the function remains the same]\n",
    "\n",
    "def calculate_time_delayed_correlations(series1: pd.Series, \n",
    "                                      series2: pd.Series, \n",
    "                                      max_lag: int = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate correlations at different time delays between two series.\n",
    "    \n",
    "    Args:\n",
    "        series1 (pd.Series): First time series\n",
    "        series2 (pd.Series): Second time series\n",
    "        max_lag (int, optional): Maximum lag to consider. Defaults to CONFIG['max_lag']\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Dictionary containing correlation results for each lag:\n",
    "            {\n",
    "                'delayed_correlations': {\n",
    "                    lag_value: {\n",
    "                        'method': {\n",
    "                            'correlation': float,\n",
    "                            'p_value': float\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "    Example:\n",
    "        >>> results = calculate_time_delayed_correlations(df['series1'], df['series2'])\n",
    "        >>> lag_0_pearson = results['delayed_correlations'][0]['pearson']['correlation']\n",
    "    \"\"\"\n",
    "    [Rest of the function remains the same]\n",
    "\n",
    "def combine_correlation_analyses(series1: pd.Series, series2: pd.Series) -> Dict:\n",
    "    \"\"\"\n",
    "    Combine all correlation analyses into a comprehensive result set.\n",
    "    \n",
    "    Args:\n",
    "        series1 (pd.Series): First time series\n",
    "        series2 (pd.Series): Second time series\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Comprehensive dictionary containing all correlation analyses:\n",
    "            {\n",
    "                'basic_correlations': Dict,  # From calculate_basic_correlations()\n",
    "                'rolling_correlation': Dict, # From calculate_rolling_correlation()\n",
    "                'ccf': Dict,                # From calculate_ccf()\n",
    "                'delayed_correlations': Dict # From calculate_time_delayed_correlations()\n",
    "            }\n",
    "            \n",
    "    Example:\n",
    "        >>> results = combine_correlation_analyses(df['series1'], df['series2'])\n",
    "        >>> ccf_lag = results['ccf']['ccf']['optimal_lag']\n",
    "        >>> basic_pearson = results['basic_correlations']['pearson']['correlation']\n",
    "    \"\"\"\n",
    "    [Rest of the function remains the same]\n",
    "\n",
    "# Example usage (commented out for library import)\n",
    "\"\"\"\n",
    "def example_advanced_correlation_analysis():\n",
    "    # Create sample data with a known lag\n",
    "    dates = pd.date_range(start='2023-01-01', periods=100, freq='D')\n",
    "    series1 = pd.Series(np.random.randn(100).cumsum(), index=dates)\n",
    "    series2 = series1.shift(3) + np.random.randn(100) * 0.1  # Series2 lags Series1 by 3 periods\n",
    "    \n",
    "    # Get comprehensive results\n",
    "    results = combine_correlation_analyses(series1, series2)\n",
    "    \n",
    "    # Print CCF results\n",
    "    print(\"\\nCCF Analysis:\")\n",
    "    print(f\"Maximum correlation: {results['ccf']['ccf']['correlation']:.4f}\")\n",
    "    print(f\"Optimal lag: {results['ccf']['ccf']['optimal_lag']}\")\n",
    "    \n",
    "    # Print best delayed correlation\n",
    "    max_delayed = max(\n",
    "        results['delayed_correlations']['delayed_correlations'].items(),\n",
    "        key=lambda x: abs(x[1]['pearson']['correlation'])\n",
    "    )\n",
    "    print(f\"\\nBest Time-Delayed Correlation:\")\n",
    "    print(f\"Lag: {max_delayed[0]}\")\n",
    "    print(f\"Correlation: {max_delayed[1]['pearson']['correlation']:.4f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cfaabc-33e2-4d44-bc27-30d12dc1e035",
   "metadata": {},
   "source": [
    "# Step 4 of 6: Analysis Framework and Method Comparison\n",
    "Dependencies: Cells 1-3 (imports, data loading, basic and advanced correlations)\n",
    "Outputs: Structured comparison framework and best-method determination\n",
    "\n",
    "Notes:\n",
    "- Compares all correlation methods\n",
    "- Determines best method per relationship\n",
    "- Handles significance testing\n",
    "- Prepares sorted results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff90ba5-5458-4224-8e98-fa0864c885a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 of 6: Analysis Framework and Method Comparison\n",
    "\n",
    "def analyze_relationship_type(results: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze and classify the type of relationship between time series variables.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Combined correlation results from previous analyses containing:\n",
    "            - basic_correlations\n",
    "            - ccf\n",
    "            - rolling_correlation\n",
    "            - delayed_correlations\n",
    "            \n",
    "    Returns:\n",
    "        Dict: Classification results with format:\n",
    "            {\n",
    "                'primary_type': str,     # One of: 'linear', 'non_linear', 'lagged', 'complex'\n",
    "                'confidence': float,      # Classification confidence score (0-1)\n",
    "                'supporting_metrics': {}, # Additional classification metrics\n",
    "                'method_recommendations': [str] # List of recommended correlation methods\n",
    "            }\n",
    "            \n",
    "    Example:\n",
    "        >>> results = combine_correlation_analyses(series1, series2)\n",
    "        >>> classification = analyze_relationship_type(results)\n",
    "        >>> print(f\"Relationship type: {classification['primary_type']}\")\n",
    "    \"\"\"\n",
    "    # Extract components from results\n",
    "    basic = results['basic_correlations']\n",
    "    ccf = results['ccf']\n",
    "    rolling = results['rolling_correlation']\n",
    "    delayed = results['delayed_correlations']\n",
    "    \n",
    "    # Calculate classification metrics\n",
    "    pearson_spearman_diff = abs(basic['pearson']['correlation'] - \n",
    "                               basic['spearman']['correlation'])\n",
    "    rolling_std = rolling['rolling_correlation']['std']\n",
    "    lag_impact = ccf['ccf']['lag_strength_ratio']\n",
    "    \n",
    "    # Initialize classification\n",
    "    classification = {\n",
    "        'primary_type': None,\n",
    "        'confidence': 0.0,\n",
    "        'supporting_metrics': {},\n",
    "        'method_recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Classify relationship type based on metrics\n",
    "    if pearson_spearman_diff < 0.1 and rolling_std < 0.2:\n",
    "        classification['primary_type'] = 'linear'\n",
    "        classification['method_recommendations'].append('pearson')\n",
    "    elif pearson_spearman_diff > 0.2:\n",
    "        classification['primary_type'] = 'non_linear'\n",
    "        classification['method_recommendations'].extend(['spearman', 'kendall'])\n",
    "    elif lag_impact > 1.2:\n",
    "        classification['primary_type'] = 'lagged'\n",
    "        classification['method_recommendations'].append('ccf')\n",
    "    else:\n",
    "        classification['primary_type'] = 'complex'\n",
    "        classification['method_recommendations'].extend(['ccf', 'spearman'])\n",
    "    \n",
    "    # Calculate confidence\n",
    "    classification['confidence'] = calculate_confidence(results)\n",
    "    \n",
    "    return classification\n",
    "\n",
    "def calculate_confidence(results: Dict) -> float:\n",
    "    \"\"\"\n",
    "    Calculate confidence score for relationship classification.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Combined correlation results\n",
    "        \n",
    "    Returns:\n",
    "        float: Confidence score between 0 and 1\n",
    "            - Higher values indicate stronger confidence in classification\n",
    "            - Based on significance tests and correlation strengths\n",
    "            \n",
    "    Example:\n",
    "        >>> results = combine_correlation_analyses(series1, series2)\n",
    "        >>> confidence = calculate_confidence(results)\n",
    "        >>> print(f\"Classification confidence: {confidence:.2f}\")\n",
    "    \"\"\"\n",
    "    basic = results['basic_correlations']\n",
    "    significant_count = sum([1 for method in basic.values() if method['significant']])\n",
    "    \n",
    "    # Calculate confidence based on significance and correlation strength\n",
    "    confidence = (significant_count / 3) * \\\n",
    "                 max(abs(basic['pearson']['correlation']),\n",
    "                     abs(basic['spearman']['correlation']),\n",
    "                     abs(basic['kendall']['correlation']))\n",
    "    \n",
    "    return round(confidence, 3)\n",
    "\n",
    "def create_summary_table(series1_name: str, series2_name: str, \n",
    "                        results: Dict, classification: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a comprehensive summary table of all correlation analyses.\n",
    "    \n",
    "    Args:\n",
    "        series1_name (str): Name of first time series\n",
    "        series2_name (str): Name of second time series\n",
    "        results (Dict): Combined correlation results\n",
    "        classification (Dict): Relationship classification results\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Summary table with columns:\n",
    "            - Series names\n",
    "            - Relationship type\n",
    "            - Confidence\n",
    "            - Best methods\n",
    "            - Correlation values for each method\n",
    "            - CCF and lag information\n",
    "            - Rolling correlation statistics\n",
    "            \n",
    "    Example:\n",
    "        >>> results = combine_correlation_analyses(series1, series2)\n",
    "        >>> classification = analyze_relationship_type(results)\n",
    "        >>> summary = create_summary_table('GDP', 'Unemployment', results, classification)\n",
    "        >>> print(summary)\n",
    "    \"\"\"\n",
    "    summary = {\n",
    "        'Series 1': series1_name,\n",
    "        'Series 2': series2_name,\n",
    "        'Relationship Type': classification['primary_type'],\n",
    "        'Confidence': classification['confidence'],\n",
    "        'Best Method': ', '.join(classification['method_recommendations']),\n",
    "        'Pearson': results['basic_correlations']['pearson']['correlation'],\n",
    "        'Spearman': results['basic_correlations']['spearman']['correlation'],\n",
    "        'Kendall': results['basic_correlations']['kendall']['correlation'],\n",
    "        'Max CCF': results['ccf']['ccf']['correlation'],\n",
    "        'Optimal Lag': results['ccf']['ccf']['optimal_lag'],\n",
    "        'Rolling Mean': results['rolling_correlation']['rolling_correlation']['mean']\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame([summary])\n",
    "\n",
    "# Example usage (commented out for library import)\n",
    "\"\"\"\n",
    "def example_analysis_framework():\n",
    "    # Create sample data with known relationship type\n",
    "    dates = pd.date_range(start='2023-01-01', periods=100, freq='D')\n",
    "    x = np.random.randn(100).cumsum()\n",
    "    \n",
    "    # Linear relationship\n",
    "    y_linear = x + np.random.randn(100) * 0.1\n",
    "    \n",
    "    # Non-linear relationship\n",
    "    y_nonlinear = np.exp(x/10) + np.random.randn(100) * 0.1\n",
    "    \n",
    "    # Create series\n",
    "    series1 = pd.Series(y_linear, index=dates)\n",
    "    series2 = pd.Series(y_nonlinear, index=dates)\n",
    "    \n",
    "    # Analyze relationship\n",
    "    results = combine_correlation_analyses(series1, series2)\n",
    "    classification = analyze_relationship_type(results)\n",
    "    summary = create_summary_table('Linear_Series', 'Nonlinear_Series', \n",
    "                                 results, classification)\n",
    "    \n",
    "    print(\"\\nRelationship Analysis Results:\")\n",
    "    print(summary.to_string())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1985199-3f5b-45b2-9312-c1fa416f2dbb",
   "metadata": {},
   "source": [
    "# Step 5 of 6: Visualization Functions\n",
    "Dependencies: Cells 1-4 (all previous analyses)\n",
    "Outputs: Multi-method visualization suite\n",
    "\n",
    "Notes:\n",
    "- Creates comparative visualizations\n",
    "- Shows method-specific patterns\n",
    "- Highlights relationship classifications\n",
    "- Supports decision-making for method selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe885bff-0f36-4db3-b942-2313087188c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 of 6: Visualization Functions\n",
    "\n",
    "def create_method_comparison_plot(series1: pd.Series, series2: pd.Series, \n",
    "                                results: Dict, classification: Dict,\n",
    "                                series1_name: str = 'Series 1', \n",
    "                                series2_name: str = 'Series 2') -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Create a comprehensive visualization suite comparing all correlation methods.\n",
    "    \n",
    "    Args:\n",
    "        series1 (pd.Series): First time series\n",
    "        series2 (pd.Series): Second time series\n",
    "        results (Dict): Combined correlation results from combine_correlation_analyses()\n",
    "        classification (Dict): Relationship classification from analyze_relationship_type()\n",
    "        series1_name (str, optional): Name of first series. Defaults to 'Series 1'\n",
    "        series2_name (str, optional): Name of second series. Defaults to 'Series 2'\n",
    "    \n",
    "    Returns:\n",
    "        plt.Figure: Figure containing six subplots:\n",
    "            1. Normalized time series comparison\n",
    "            2. Rolling correlation\n",
    "            3. Cross-correlation function\n",
    "            4. Method comparison bar chart\n",
    "            5. Time-delayed correlations\n",
    "            6. Method recommendations and metrics\n",
    "            \n",
    "    Example:\n",
    "        >>> results = combine_correlation_analyses(series1, series2)\n",
    "        >>> classification = analyze_relationship_type(results)\n",
    "        >>> fig = create_method_comparison_plot(\n",
    "        ...     series1, series2, \n",
    "        ...     results, \n",
    "        ...     classification,\n",
    "        ...     'GDP', 'Unemployment'\n",
    "        ... )\n",
    "        >>> plt.show()\n",
    "    \"\"\"\n",
    "    [original plotting code remains the same]\n",
    "\n",
    "# Example usage (commented out for library import)\n",
    "\"\"\"\n",
    "def example_visualization():\n",
    "    # Create sample data\n",
    "    dates = pd.date_range(start='2023-01-01', periods=100, freq='D')\n",
    "    \n",
    "    # Generate three different types of relationships\n",
    "    \n",
    "    # 1. Linear relationship\n",
    "    x = np.random.randn(100).cumsum()\n",
    "    y_linear = x + np.random.randn(100) * 0.1\n",
    "    \n",
    "    # 2. Non-linear relationship\n",
    "    y_nonlinear = np.exp(x/10) + np.random.randn(100) * 0.1\n",
    "    \n",
    "    # 3. Lagged relationship\n",
    "    y_lagged = np.roll(x, 5) + np.random.randn(100) * 0.1\n",
    "    \n",
    "    # Create series\n",
    "    series_pairs = [\n",
    "        (pd.Series(x, index=dates), pd.Series(y_linear, index=dates), 'Linear'),\n",
    "        (pd.Series(x, index=dates), pd.Series(y_nonlinear, index=dates), 'Non-linear'),\n",
    "        (pd.Series(x, index=dates), pd.Series(y_lagged, index=dates), 'Lagged')\n",
    "    ]\n",
    "    \n",
    "    # Create visualizations for each relationship type\n",
    "    for s1, s2, relationship_type in series_pairs:\n",
    "        results = combine_correlation_analyses(s1, s2)\n",
    "        classification = analyze_relationship_type(results)\n",
    "        fig = create_method_comparison_plot(\n",
    "            s1, s2, \n",
    "            results, \n",
    "            classification,\n",
    "            f'Input_{relationship_type}', \n",
    "            f'Output_{relationship_type}'\n",
    "        )\n",
    "        plt.show()\n",
    "\"\"\"\n",
    "\n",
    "def set_visualization_style():\n",
    "    \"\"\"\n",
    "    Configure matplotlib style for consistent visualizations.\n",
    "    Call this before creating plots to ensure consistent styling.\n",
    "    \"\"\"\n",
    "    plt.style.use('seaborn')\n",
    "    plt.rcParams['figure.figsize'] = (20, 12)\n",
    "    plt.rcParams['axes.titlesize'] = 12\n",
    "    plt.rcParams['axes.labelsize'] = 10\n",
    "    plt.rcParams['xtick.labelsize'] = 9\n",
    "    plt.rcParams['ytick.labelsize'] = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7886ff0-48f6-424c-bd82-aa89f7a476b2",
   "metadata": {},
   "source": [
    "# Step 6 of 6: Results Processing and Full Dataset Analysis\n",
    "- Dependencies: Cells 1-5 (all previous analyses and visualizations)\n",
    "- Outputs: Comprehensive analysis across all valid pairs, grouped by relationship type\n",
    "\n",
    "Notes:\n",
    "- Processes all valid pairs\n",
    "- Groups by relationship type\n",
    "- Sorts by correlation strength\n",
    "- Generates summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba7a2e1-12f3-430a-8742-b715e3d7780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 of 6: Results Processing and Full Dataset Analysis\n",
    "\n",
    "def analyze_full_dataset(df: pd.DataFrame, \n",
    "                        valid_pairs: List[Tuple[str, str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform comprehensive correlation analysis on all valid pairs in dataset.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing time series columns\n",
    "        valid_pairs (List[Tuple[str, str]]): List of valid column pairs to compare\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Results DataFrame with columns:\n",
    "            - Series 1, Series 2: Names of compared series\n",
    "            - Relationship Type: Classification of relationship\n",
    "            - Confidence: Classification confidence score\n",
    "            - Best Method: Recommended correlation method(s)\n",
    "            - Pearson, Spearman, Kendall: Correlation coefficients\n",
    "            - Max CCF: Maximum cross-correlation\n",
    "            - Optimal Lag: Lag with maximum correlation\n",
    "            - Rolling Mean: Average rolling correlation\n",
    "            - Abs_Max_Corr: Maximum absolute correlation across methods\n",
    "            \n",
    "    Example:\n",
    "        >>> valid_pairs = [('GDP', 'Unemployment'), ('Inflation', 'Interest_Rate')]\n",
    "        >>> results = analyze_full_dataset(economic_data, valid_pairs)\n",
    "        >>> print(results.sort_values('Abs_Max_Corr', ascending=False))\n",
    "    \"\"\"\n",
    "    [original analysis code remains the same]\n",
    "\n",
    "def generate_summary_statistics(results_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Generate summary statistics from analysis results.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pd.DataFrame): Results from analyze_full_dataset()\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Summary statistics including:\n",
    "            - relationship_types: Count of each relationship type\n",
    "            - avg_confidence: Average confidence score\n",
    "            - method_counts: Frequency of recommended methods\n",
    "            - strong_correlations: Count of correlations > 0.7\n",
    "            - moderate_correlations: Count of correlations 0.3-0.7\n",
    "            - weak_correlations: Count of correlations < 0.3\n",
    "            \n",
    "    Example:\n",
    "        >>> stats = generate_summary_statistics(results_df)\n",
    "        >>> print(f\"Found {stats['strong_correlations']} strong correlations\")\n",
    "    \"\"\"\n",
    "    [original statistics code remains the same]\n",
    "\n",
    "def print_grouped_results(results_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Print analysis results grouped by relationship type and sorted by strength.\n",
    "    \n",
    "    Args:\n",
    "        results_df (pd.DataFrame): Results from analyze_full_dataset()\n",
    "        \n",
    "    Prints:\n",
    "        - Results grouped by relationship type\n",
    "        - Top 5 strongest correlations per type\n",
    "        - Summary statistics for each group\n",
    "        \n",
    "    Example:\n",
    "        >>> print_grouped_results(results_df)\n",
    "        === LINEAR RELATIONSHIPS ===\n",
    "        Number of pairs: 15\n",
    "        Top 5 strongest correlations:\n",
    "        [table of results]\n",
    "    \"\"\"\n",
    "    [original printing code remains the same]\n",
    "\n",
    "def run_full_analysis(df: pd.DataFrame, \n",
    "                     valid_pairs: List[Tuple[str, str]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run complete TRACES analysis pipeline on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame containing time series columns\n",
    "        valid_pairs (List[Tuple[str, str]]): List of valid column pairs to compare\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Complete analysis results\n",
    "        \n",
    "    Prints:\n",
    "        - Analysis summary\n",
    "        - Relationship type distribution\n",
    "        - Confidence scores\n",
    "        - Correlation strength distribution\n",
    "        - Grouped results by relationship type\n",
    "        \n",
    "    Example:\n",
    "        >>> # Create sample dataset\n",
    "        >>> dates = pd.date_range('2023-01-01', periods=100)\n",
    "        >>> data = pd.DataFrame({\n",
    "        ...     'A': np.random.randn(100).cumsum(),\n",
    "        ...     'B': np.random.randn(100).cumsum(),\n",
    "        ...     'C': np.random.randn(100).cumsum()\n",
    "        ... }, index=dates)\n",
    "        >>> valid_pairs = [('A', 'B'), ('B', 'C'), ('A', 'C')]\n",
    "        >>> results = run_full_analysis(data, valid_pairs)\n",
    "    \"\"\"\n",
    "    print(\"Starting full dataset analysis...\")\n",
    "    \n",
    "    # Analyze all pairs\n",
    "    full_results = analyze_full_dataset(df, valid_pairs)\n",
    "    \n",
    "    # Generate and print summary statistics\n",
    "    summary_stats = generate_summary_statistics(full_results)\n",
    "    \n",
    "    print(\"\\nANALYSIS SUMMARY:\")\n",
    "    print(f\"Total pairs analyzed: {len(full_results)}\")\n",
    "    print(\"\\nRelationship Types Distribution:\")\n",
    "    for rel_type, count in summary_stats['relationship_types'].items():\n",
    "        print(f\"{rel_type}: {count}\")\n",
    "    \n",
    "    print(\"\\nAverage Confidence Score:\", \n",
    "          f\"{summary_stats['avg_confidence']:.3f}\")\n",
    "    \n",
    "    print(\"\\nCorrelation Strength Distribution:\")\n",
    "    print(f\"Strong correlations (>0.7): {summary_stats['strong_correlations']}\")\n",
    "    print(f\"Moderate correlations (0.3-0.7): \"\n",
    "          f\"{summary_stats['moderate_correlations']}\")\n",
    "    print(f\"Weak correlations (<0.3): {summary_stats['weak_correlations']}\")\n",
    "    \n",
    "    # Print grouped results\n",
    "    print_grouped_results(full_results)\n",
    "    \n",
    "    return full_results\n",
    "\n",
    "# Example usage (commented out for library import)\n",
    "\"\"\"\n",
    "def example_full_analysis():\n",
    "    # Create sample dataset\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2023-01-01', periods=100)\n",
    "    \n",
    "    # Generate different types of relationships\n",
    "    base = np.random.randn(100).cumsum()\n",
    "    data = pd.DataFrame({\n",
    "        'Linear': base + np.random.randn(100) * 0.1,\n",
    "        'NonLinear': np.exp(base/10),\n",
    "        'Lagged': np.roll(base, 5),\n",
    "        'Random': np.random.randn(100)\n",
    "    }, index=dates)\n",
    "    \n",
    "    # Define valid pairs\n",
    "    valid_pairs = [\n",
    "        ('Linear', 'NonLinear'),\n",
    "        ('Linear', 'Lagged'),\n",
    "        ('Linear', 'Random'),\n",
    "        ('NonLinear', 'Lagged'),\n",
    "        ('NonLinear', 'Random'),\n",
    "        ('Lagged', 'Random')\n",
    "    ]\n",
    "    \n",
    "    # Run analysis\n",
    "    results = run_full_analysis(data, valid_pairs)\n",
    "    return results\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402ade01-9940-41e1-ba51-ee22f5354410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
