{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea9f700-1b97-4f4c-b84e-2247398677e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# TRACES (Time-series Relationship Analysis with Comprehensive Evaluation Suite)\n",
    "\n",
    "---\n",
    "\n",
    "_A Hierarchical Multi-Method Time Series Correlation Analyzer_\n",
    "\n",
    "## Overview\n",
    "\n",
    "TRACES is a comprehensive framework for analyzing relationships between time series data using multiple correlation methods. It automatically determines the most appropriate correlation method(s) for each pair of series and provides detailed visualizations and analysis.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- Multi-method correlation analysis\n",
    "  - Pearson correlation\n",
    "  - Spearman rank correlation\n",
    "  - Kendall's Tau\n",
    "  - Cross-Correlation Function (CCF)\n",
    "  - Rolling window correlations\n",
    "- Automatic relationship type classification\n",
    "  - **Linear** relationships\n",
    "  - **Non-linear** relationships\n",
    "  - **Lagged** relationships\n",
    "  - **Complex** relationships\n",
    "- Confidence scoring system\n",
    "- Advanced visualization suite\n",
    "- Comprehensive statistical testing\n",
    "- Parent-child relationship handling\n",
    "\n",
    "## Input Requirements\n",
    "\n",
    "- Excel file (.xlsx)\n",
    "- First row: Column headers (series labels)\n",
    "- First column: Time intervals\n",
    "- Additional columns: Time series data\n",
    "- Minimum 3 data points per series\n",
    "- Numeric data only (except time labels)\n",
    "\n",
    "## Analysis Pipeline\n",
    "\n",
    "1. **Setup and Configuration**\n",
    "   - Environment initialization\n",
    "   - Data loading and validation\n",
    "   - Parent-child relationship definition\n",
    "\n",
    "2. **Core Correlation Analysis**\n",
    "   - Basic correlation calculations\n",
    "   - Significance testing\n",
    "   - Method comparison framework\n",
    "\n",
    "3. **Advanced Analysis**\n",
    "   - Cross-correlation analysis\n",
    "   - Time-delayed correlations\n",
    "   - Rolling window analysis\n",
    "\n",
    "4. **Relationship Classification**\n",
    "   - Type determination\n",
    "   - Confidence scoring\n",
    "   - Method recommendations\n",
    "\n",
    "5. **Visualization**\n",
    "   - Correlation method comparisons\n",
    "   - Relationship matrix heatmaps\n",
    "   - Method performance analysis\n",
    "   - CCF and lag pattern visualization\n",
    "\n",
    "6. **Results Processing**\n",
    "   - Comprehensive summary statistics\n",
    "   - Grouped relationship analysis\n",
    "   - Strength distribution reports\n",
    "\n",
    "## Output Components\n",
    "\n",
    "1. **Correlation Analysis**\n",
    "   - Basic correlations with significance tests\n",
    "   - Time-delayed correlation patterns\n",
    "   - Rolling correlation trends\n",
    "   - Cross-correlation results\n",
    "\n",
    "2. **Classification Results**\n",
    "   - Relationship type identification\n",
    "   - Confidence scores\n",
    "   - Method recommendations\n",
    "   - Supporting metrics\n",
    "\n",
    "3. **Visualization Suite**\n",
    "   - Interactive correlation comparisons\n",
    "   - Relationship type matrix (optimized for top $N$ even-numbered pairs)\n",
    "     - Includes strategic NaN visualization for unpaired relationships\n",
    "     - Highlights strongest relationship patterns effectively\n",
    "   - Method performance charts\n",
    "   - CCF pattern analysis\n",
    "\n",
    "4. **Summary Statistics**\n",
    "   - Relationship type distribution\n",
    "   - Correlation strength metrics\n",
    "   - Confidence score analysis\n",
    "   - Method effectiveness summary\n",
    "\n",
    "## Technical Dependencies\n",
    "\n",
    "- Python 3.x\n",
    "- Core libraries:\n",
    "  - pandas\n",
    "  - numpy\n",
    "  - scipy\n",
    "  - matplotlib\n",
    "  - seaborn\n",
    "\n",
    "## Performance Considerations\n",
    "\n",
    "- Optimized for datasets with up to 1000's of pair comparisons\n",
    "- Automatic handling of missing values\n",
    "- Efficient parent-child relationship exclusion\n",
    "- Scalable visualization components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a16199-d9f7-4dae-85bf-10feb9f39377",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Component Flexibility Guide:\n",
    "----------------------------\n",
    "\n",
    "---\n",
    "\n",
    "### **GREEN** ZONE (Highly Customizable)\n",
    "- Configuration parameters\n",
    "  - Rolling window size\n",
    "  - Maximum lag\n",
    "  - Significance levels\n",
    "  - Correlation thresholds\n",
    "- Visualization settings\n",
    "- Output format preferences\n",
    "- Parent-child definitions\n",
    "\n",
    "### **YELLOW** ZONE (Modify with Caution)\n",
    "- Classification thresholds\n",
    "- Confidence scoring parameters\n",
    "- CCF analysis settings\n",
    "- Method comparison logic\n",
    "\n",
    "### **RED** ZONE (Core Framework)\n",
    "- Base correlation algorithms\n",
    "- Statistical testing methods\n",
    "- Data structure handling\n",
    "- Core analysis pipeline\n",
    "\n",
    "## Usage Notes\n",
    "\n",
    "- Handles missing values automatically\n",
    "- Supports various time series lengths\n",
    "- Provides both pair-wise and full dataset analysis\n",
    "- Includes robust error handling\n",
    "- Generates reproducible results\n",
    "\n",
    "## Critical Requirements\n",
    "\n",
    "1. Data Structure\n",
    "   - Continuous time series\n",
    "   - Ordered intervals\n",
    "   - Consistent column names\n",
    "   - Numeric values\n",
    "\n",
    "2. Parent-Child Relationships\n",
    "   - Explicit mapping required\n",
    "   - Valid pair generation\n",
    "   - Automatic exclusion handling\n",
    "\n",
    "3. Statistical Validity\n",
    "   - Minimum sample size requirements\n",
    "   - Significance testing\n",
    "   - Confidence scoring\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. Data Preparation\n",
    "   - Clean and validate input data\n",
    "   - Check for missing values\n",
    "   - Ensure proper formatting\n",
    "\n",
    "2. Analysis Configuration\n",
    "   - Set appropriate thresholds\n",
    "   - Define parent-child relationships\n",
    "   - Configure visualization preferences\n",
    "\n",
    "3. Results Interpretation\n",
    "   - Consider confidence scores\n",
    "   - Review multiple correlation methods\n",
    "   - Examine lag patterns\n",
    "   - Validate relationship classifications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0766cac-f251-4862-a55b-a1154c25f02b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# TRACES Operational Guide\n",
    "\n",
    "---\n",
    "\n",
    "## Cell Dependencies and Execution Flows\n",
    "\n",
    "### Initial Setup/Modification\n",
    "\n",
    "**Full Sequential Execution (1-6) Required When:**\n",
    "- Performing first-time setup\n",
    "- Modifying any functions\n",
    "- Adjusting core parameters\n",
    "- Implementing new methods\n",
    "- Updating visualization components\n",
    "\n",
    "### Standard Analysis Workflows\n",
    "\n",
    "#### Minimum Required Flow\n",
    "1. **Cell 1** (Setup & Environment) - *Always Required*\n",
    "2. **Cell 6** (Full Analysis) - *Primary Execution*\n",
    "\n",
    "#### Targeted Analysis Options\n",
    "\n",
    "**Visualization Focus:**\n",
    "- Cell 1 → Cell 5\n",
    "- Enables all visualization capabilities\n",
    "- Requires relationship matrix parameter alignment (even number of top pairs)\n",
    "\n",
    "**Method Comparison:**\n",
    "- Cell 1 → Cell 4\n",
    "- Focuses on correlation method analysis\n",
    "\n",
    "### Use Case Scenarios\n",
    "\n",
    "#### A. Complete Dataset Analysis\n",
    "1. Sequential execution: Cells 1 → 2 → 3 → 4 → 5 → 6\n",
    "2. Provides:\n",
    "   - Comprehensive correlation analysis\n",
    "   - Full visualization suite\n",
    "   - Detailed statistical insights\n",
    "   - Relationship classifications\n",
    "\n",
    "#### B. Visualization Exploration\n",
    "1. Execute: Cell 1 → Cell 4 → Cell 5\n",
    "2. Delivers:\n",
    "   - Correlation comparisons\n",
    "   - Relationship matrix (top N pairs)\n",
    "   - Method performance analysis\n",
    "   - CCF pattern visualization\n",
    "\n",
    "#### C. Methodology Validation\n",
    "1. Required: Cells 1-4\n",
    "2. Useful for:\n",
    "   - Testing correlation methods\n",
    "   - Validating classifications\n",
    "   - Assessing confidence metrics\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "**State Management:**\n",
    "- Notebook maintains state until kernel reset\n",
    "- Cell 6 contains consolidated function calls\n",
    "- Parent-child relationships persist through session\n",
    "\n",
    "**Performance Considerations:**\n",
    "- Clear outputs between analysis runs\n",
    "- Restart kernel when modifying parent-child mappings\n",
    "- Consider batch processing for large datasets\n",
    "- Relationship matrix visualization optimized for even number of top pairs\n",
    "\n",
    "**Best Practices:**\n",
    "- Validate data structure before full analysis\n",
    "- Monitor memory usage with large datasets\n",
    "- Review visualization parameters for optimal display\n",
    "- Ensure correlation pair count aligns with visualization requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338cc74-0fab-4e96-87c8-a6d606eab76f",
   "metadata": {},
   "source": [
    "# FORMULAE\n",
    "\n",
    "---\n",
    "\n",
    "# 1. **Pearson Correlation Coefficient** ($r$)\n",
    "\n",
    "#### Formula:\n",
    "\n",
    "$$\n",
    "r = \\frac{\\sum_{i=1}^n \\left(X_i - \\bar{X}\\right)\\left(Y_i - \\bar{Y}\\right)}{\\sqrt{\\sum_{i=1}^n \\left(X_i - \\bar{X}\\right)^2} \\sqrt{\\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2}}\n",
    "$$\n",
    "\n",
    "##### _Where_:\n",
    "\n",
    "- $X_i$ and $Y_i$ are individual sample points.\n",
    "- $\\bar{X}$ and $\\bar{Y}$ are the means of the $X$ and $Y$ samples, respectively.\n",
    "- $n$ is the number of paired samples.\n",
    "\n",
    "# 2. **Spearman's Rank Correlation Coefficient** ($\\rho$)\n",
    "\n",
    "#### Formula:\n",
    "\n",
    "$$\n",
    "\\rho = 1 - \\frac{6 \\sum_{i=1}^n d_i^2}{n\\left(n^2 - 1\\right)}\n",
    "$$\n",
    "\n",
    "##### _Where_:\n",
    "\n",
    "- $d_i = \\operatorname{rank}(X_i) - \\operatorname{rank}(Y_i)$ is the difference between the ranks of corresponding variables.\n",
    "- $n$ is the number of observations.\n",
    "\n",
    "# 3. **Kendall's Tau** ($\\tau$)\n",
    "\n",
    "#### Formula:\n",
    "\n",
    "$$\n",
    "\\tau = \\frac{C - D}{\\sqrt{(C + D + X)(C + D + Y)}}\n",
    "$$\n",
    "\n",
    "##### _Where_:\n",
    "\n",
    "- $C$ is the number of concordant pairs.\n",
    "- $D$ is the number of discordant pairs.\n",
    "- $X$ is the number of pairs tied only in $X$.\n",
    "- $Y$ is the number of pairs tied only in $Y$.\n",
    "\n",
    "# 4. **Cross-Correlation Function** $\\text{CCF}(k)$\n",
    "\n",
    "#### Formula:\n",
    "\n",
    "$$\n",
    "\\text{CCF}(k) = \\frac{\\sum_{i=1}^{n - k} \\left(X_i - \\bar{X}\\right)\\left(Y_{i + k} - \\bar{Y}\\right)}{\\sqrt{\\sum_{i=1}^n \\left(X_i - \\bar{X}\\right)^2} \\sqrt{\\sum_{i=1}^n \\left(Y_i - \\bar{Y}\\right)^2}}\n",
    "$$\n",
    "\n",
    "##### _Where_:\n",
    "\n",
    "- $X_i$ and $Y_i$ are individual sample points from sequences $X$ and $Y$, respectively.\n",
    "- $\\bar{X}$ and $\\bar{Y}$ are the means of sequences $X$ and $Y$.\n",
    "- $n$ is the length of the series.\n",
    "- $k$ is the lag (an integer representing the shift between the series).\n",
    "- $\\text{CCF}(k)$ represents the cross-correlation at lag $k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee16a72-f209-46e1-96c8-1c84bfdc1c4f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Step 1 of 6: Setup and Environment Configuration\n",
    "\n",
    "- Dependencies: None\n",
    "- Outputs: Configured environment with required libraries and global parameters\n",
    "\n",
    "### Description:\n",
    "- Initializes core dependencies\n",
    "- Establishes data structures\n",
    "- Defines parent-child relationships\n",
    "- Sets global analysis parameters\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b28861-7979-43b4-b143-e02346455dd3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1 of 6: Setup and Environment Configuration\n",
    "\n",
    "\"\"\"TRACES Setup and Environment Configuration.\n",
    "\n",
    "This module initializes the TRACES framework environment, loads required libraries,\n",
    "and sets up core data structures for time series relationship analysis.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Core configuration parameters\n",
    "PARENT_CHILD_MAPPING: Dict[str, List[str]] = {}\n",
    "\n",
    "CONFIG = {\n",
    "    'rolling_window': 12,      # Window size for rolling correlations\n",
    "    'max_lag': 10,            # Maximum lag for time-delayed analysis\n",
    "    'significance_level': 0.05,  # Statistical significance threshold\n",
    "    'min_correlation': 0.3     # Minimum correlation strength threshold\n",
    "}\n",
    "\n",
    "def load_and_prepare_data(file_path: str) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Load and prepare time series data for relationship analysis.\n",
    "\n",
    "    Loads time series data from an Excel file and generates valid comparison pairs,\n",
    "    excluding defined parent-child relationships.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to Excel file (.xlsx) containing time series data.\n",
    "                  First column must contain time intervals.\n",
    "                  Other columns contain series data with headers as series names.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Processed time series data\n",
    "        List[str]: Valid comparison pairs, excluding parent-child relationships\n",
    "\n",
    "    Example:\n",
    "        df, pairs = load_and_prepare_data(\"path/to/data.xlsx\")\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path, header=0)\n",
    "    \n",
    "    all_columns = [col for col in df.columns if col != 'Time']\n",
    "    valid_pairs = []\n",
    "    \n",
    "    for i, col1 in enumerate(all_columns):\n",
    "        for col2 in all_columns[i+1:]:\n",
    "            is_parent_child = False\n",
    "            for parent, children in PARENT_CHILD_MAPPING.items():\n",
    "                if (col1 == parent and col2 in children) or \\\n",
    "                   (col2 == parent and col1 in children):\n",
    "                    is_parent_child = True\n",
    "                    break\n",
    "            \n",
    "            if not is_parent_child:\n",
    "                valid_pairs.append((col1, col2))\n",
    "    \n",
    "    return df, valid_pairs\n",
    "\n",
    "def normalize_series(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalize a time series to zero mean and unit variance.\n",
    "\n",
    "    Args:\n",
    "        series: Input time series data\n",
    "\n",
    "    Returns:\n",
    "        Normalized series (mean=0, std=1)\n",
    "    \"\"\"\n",
    "    return (series - series.mean()) / series.std()\n",
    "\n",
    "# Data loading validation\n",
    "try:\n",
    "    file_path = '../data/examples/TRACES_sample_52x10_dataset_A1.xlsx'\n",
    "    df, valid_pairs = load_and_prepare_data(file_path)\n",
    "    print(f\"Successfully loaded data with {len(df)} rows and {len(df.columns)} columns\")\n",
    "    print(f\"Generated {len(valid_pairs)} valid comparison pairs\")\n",
    "    \n",
    "    print(\"\\nFirst 5 comparison pairs:\")\n",
    "    for pair in valid_pairs[:5]:\n",
    "        print(pair)\n",
    "    \n",
    "    print(\"\\nColumns in dataset:\")\n",
    "    print(df.columns.tolist())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed97a9d-339d-4476-abfb-c3f500988670",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Step 2 of 6: Core Correlation Functions\n",
    "\n",
    "- Dependencies: Cell 1 (environment setup)\n",
    "- Outputs: Basic correlation framework and comparative metrics\n",
    "\n",
    "### Description:\n",
    "\n",
    "- Implements foundational correlation methods\n",
    "  - Pearson correlation\n",
    "  - Spearman's rank correlation\n",
    "  - Kendall's Tau\n",
    "- Conducts significance testing\n",
    "- Prepares standardized comparison framework\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b1f029-aafd-4975-9185-18c3c71c45aa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2 of 6: Core Correlation Functions\n",
    "\n",
    "\"\"\"TRACES Core Correlation Functions.\n",
    "\n",
    "Implements core correlation analysis methods including Pearson, Spearman, and Kendall\n",
    "correlations, with rolling window analysis and method comparison capabilities.\n",
    "\"\"\"\n",
    "\n",
    "def calculate_basic_correlations(series1: pd.Series, series2: pd.Series) -> Dict:\n",
    "    \"\"\"Calculate standard correlation measures between two time series.\n",
    "\n",
    "    Computes Pearson, Spearman, and Kendall correlations with significance testing.\n",
    "\n",
    "    Args:\n",
    "        series1: First time series data\n",
    "        series2: Second time series data\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of correlation results for each method:\n",
    "        {method_name: {correlation, p_value, significant}}\n",
    "    \"\"\"\n",
    "    s1_norm = normalize_series(series1)\n",
    "    s2_norm = normalize_series(series2)\n",
    "    \n",
    "    pearson_corr, pearson_p = pearsonr(s1_norm, s2_norm)\n",
    "    spearman_corr, spearman_p = spearmanr(s1_norm, s2_norm)\n",
    "    kendall_corr, kendall_p = kendalltau(s1_norm, s2_norm)\n",
    "    \n",
    "    return {\n",
    "        'pearson': {\n",
    "            'correlation': pearson_corr,\n",
    "            'p_value': pearson_p,\n",
    "            'significant': pearson_p < CONFIG['significance_level']\n",
    "        },\n",
    "        'spearman': {\n",
    "            'correlation': spearman_corr,\n",
    "            'p_value': spearman_p,\n",
    "            'significant': spearman_p < CONFIG['significance_level']\n",
    "        },\n",
    "        'kendall': {\n",
    "            'correlation': kendall_corr,\n",
    "            'p_value': kendall_p,\n",
    "            'significant': kendall_p < CONFIG['significance_level']\n",
    "        }\n",
    "    }\n",
    "\n",
    "def calculate_rolling_correlation(series1: pd.Series, series2: pd.Series) -> Dict:\n",
    "    \"\"\"Calculate rolling window correlations between time series.\n",
    "\n",
    "    Args:\n",
    "        series1: First time series data\n",
    "        series2: Second time series data\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing rolling correlation statistics:\n",
    "        {values, mean, std, max, min}\n",
    "    \"\"\"\n",
    "    s1_norm = normalize_series(series1)\n",
    "    s2_norm = normalize_series(series2)\n",
    "    \n",
    "    rolling_pearson = pd.Series(s1_norm).rolling(window=CONFIG['rolling_window'])\\\n",
    "        .corr(pd.Series(s2_norm))\n",
    "    \n",
    "    return {\n",
    "        'rolling_correlation': {\n",
    "            'values': rolling_pearson,\n",
    "            'mean': rolling_pearson.mean(),\n",
    "            'std': rolling_pearson.std(),\n",
    "            'max': rolling_pearson.max(),\n",
    "            'min': rolling_pearson.min()\n",
    "        }\n",
    "    }\n",
    "\n",
    "def identify_best_correlation_method(results: Dict) -> Tuple[str, float]:\n",
    "    \"\"\"Determine the correlation method showing strongest relationship.\n",
    "\n",
    "    Args:\n",
    "        results: Dictionary of correlation results from calculate_basic_correlations()\n",
    "\n",
    "    Returns:\n",
    "        (method_name, correlation_value) of strongest correlation\n",
    "    \"\"\"\n",
    "    methods = {\n",
    "        'pearson': abs(results['pearson']['correlation']),\n",
    "        'spearman': abs(results['spearman']['correlation']),\n",
    "        'kendall': abs(results['kendall']['correlation'])\n",
    "    }\n",
    "    \n",
    "    best_method = max(methods.items(), key=lambda x: x[1])\n",
    "    return best_method[0], best_method[1]\n",
    "\n",
    "# Results compilation and analysis\n",
    "print(\"Testing correlation functions across all series pairs...\")\n",
    "\n",
    "summary_results = []\n",
    "\n",
    "for pair in valid_pairs:\n",
    "    series1 = df[pair[0]]\n",
    "    series2 = df[pair[1]]\n",
    "    \n",
    "    basic_results = calculate_basic_correlations(series1, series2)\n",
    "    rolling_results = calculate_rolling_correlation(series1, series2)\n",
    "    best_method, best_value = identify_best_correlation_method(basic_results)\n",
    "    \n",
    "    summary_results.append({\n",
    "        'Series 1': pair[0],\n",
    "        'Series 2': pair[1],\n",
    "        'Pearson': basic_results['pearson']['correlation'],\n",
    "        'Pearson_Sig': basic_results['pearson']['significant'],\n",
    "        'Spearman': basic_results['spearman']['correlation'],\n",
    "        'Spearman_Sig': basic_results['spearman']['significant'],\n",
    "        'Kendall': basic_results['kendall']['correlation'],\n",
    "        'Kendall_Sig': basic_results['kendall']['significant'],\n",
    "        'Rolling_Mean': rolling_results['rolling_correlation']['mean'],\n",
    "        'Rolling_Std': rolling_results['rolling_correlation']['std'],\n",
    "        'Best_Method': best_method,\n",
    "        'Best_Value': best_value\n",
    "    })\n",
    "\n",
    "# Results analysis and display\n",
    "results_df = pd.DataFrame(summary_results)\n",
    "results_df['Abs_Best_Value'] = abs(results_df['Best_Value'])\n",
    "results_df = results_df.sort_values('Abs_Best_Value', ascending=False)\n",
    "\n",
    "print(\"\\nTop 5 Strongest Correlations:\")\n",
    "print(results_df[['Series 1', 'Series 2', 'Best_Method', 'Best_Value']].head())\n",
    "\n",
    "print(\"\\nCorrelation Method Distribution:\")\n",
    "print(results_df['Best_Method'].value_counts())\n",
    "\n",
    "print(\"\\nSignificant Correlations Count:\")\n",
    "print(f\"Pearson: {results_df['Pearson_Sig'].sum()}\")\n",
    "print(f\"Spearman: {results_df['Spearman_Sig'].sum()}\")\n",
    "print(f\"Kendall: {results_df['Kendall_Sig'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541f7ff5-cdeb-46f5-9818-b710d70e9669",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Step 3 of 6: Advanced Correlation Methods\n",
    "\n",
    "- Dependencies: Cell 1 (environment setup)\n",
    "- Outputs: Advanced correlation analysis including CCF and time-delayed metrics\n",
    "\n",
    "### Description:\n",
    "\n",
    "- Implements CCF analysis\n",
    "- Calculates time-delayed correlations\n",
    "- Performs comprehensive statistical testing\n",
    "- Prepares method comparison metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5833c6-54c4-4aef-82b5-55135900d5c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 3 of 6: Advanced Correlation Methods and CCF Analysis\n",
    "\n",
    "\"\"\"TRACES Advanced Correlation Analysis.\n",
    "\n",
    "Implements advanced time series correlation methods including Cross-Correlation Function (CCF)\n",
    "and time-delayed correlation analysis with comprehensive relationship metrics.\n",
    "\"\"\"\n",
    "\n",
    "def calculate_ccf(series1: pd.Series, series2: pd.Series, max_lag: int = None) -> Dict:\n",
    "    \"\"\"Calculate Cross Correlation Function between time series.\n",
    "\n",
    "    Args:\n",
    "        series1: First time series data\n",
    "        series2: Second time series data\n",
    "        max_lag: Maximum lag to consider (defaults to CONFIG['max_lag'])\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing CCF analysis:\n",
    "        {correlation, optimal_lag, zero_lag_correlation, \n",
    "         all_correlations, all_lags, lag_strength_ratio}\n",
    "    \"\"\"\n",
    "    if max_lag is None:\n",
    "        max_lag = CONFIG['max_lag']\n",
    "    \n",
    "    s1_norm = normalize_series(series1)\n",
    "    s2_norm = normalize_series(series2)\n",
    "    \n",
    "    correlation = signal.correlate(s1_norm, s2_norm, mode='full')\n",
    "    lags = signal.correlation_lags(len(s1_norm), len(s2_norm))\n",
    "    \n",
    "    max_corr_idx = np.argmax(np.abs(correlation))\n",
    "    max_corr = correlation[max_corr_idx]\n",
    "    max_lag_found = lags[max_corr_idx]\n",
    "    \n",
    "    central_idx = len(correlation) // 2\n",
    "    zero_lag_corr = correlation[central_idx]\n",
    "    \n",
    "    valid_range = (lags >= -max_lag) & (lags <= max_lag)\n",
    "    filtered_corr = correlation[valid_range]\n",
    "    filtered_lags = lags[valid_range]\n",
    "    \n",
    "    return {\n",
    "        'ccf': {\n",
    "            'correlation': max_corr,\n",
    "            'optimal_lag': max_lag_found,\n",
    "            'zero_lag_correlation': zero_lag_corr,\n",
    "            'all_correlations': filtered_corr,\n",
    "            'all_lags': filtered_lags,\n",
    "            'lag_strength_ratio': abs(max_corr / zero_lag_corr) if zero_lag_corr != 0 else np.inf\n",
    "        }\n",
    "    }\n",
    "\n",
    "def calculate_time_delayed_correlations(series1: pd.Series, series2: pd.Series, \n",
    "                                    max_lag: int = None) -> Dict:\n",
    "    \"\"\"Calculate correlations at different time delays.\n",
    "\n",
    "    Args:\n",
    "        series1: First time series data\n",
    "        series2: Second time series data\n",
    "        max_lag: Maximum lag to consider (defaults to CONFIG['max_lag'])\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of correlation results for each lag:\n",
    "        {lag_value: {method: {correlation, p_value}}}\n",
    "    \"\"\"\n",
    "    if max_lag is None:\n",
    "        max_lag = CONFIG['max_lag']\n",
    "    \n",
    "    results = {'delayed_correlations': {}}\n",
    "    \n",
    "    for lag in range(-max_lag, max_lag + 1):\n",
    "        if lag < 0:\n",
    "            s1 = series1.iloc[abs(lag):]\n",
    "            s2 = series2.iloc[:lag]\n",
    "        elif lag > 0:\n",
    "            s1 = series1.iloc[:-lag]\n",
    "            s2 = series2.iloc[lag:]\n",
    "        else:\n",
    "            s1 = series1\n",
    "            s2 = series2\n",
    "            \n",
    "        pearson_corr, pearson_p = pearsonr(s1, s2)\n",
    "        spearman_corr, spearman_p = spearmanr(s1, s2)\n",
    "        kendall_corr, kendall_p = kendalltau(s1, s2)\n",
    "        \n",
    "        results['delayed_correlations'][lag] = {\n",
    "            'pearson': {'correlation': pearson_corr, 'p_value': pearson_p},\n",
    "            'spearman': {'correlation': spearman_corr, 'p_value': spearman_p},\n",
    "            'kendall': {'correlation': kendall_corr, 'p_value': kendall_p}\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def combine_correlation_analyses(series1: pd.Series, series2: pd.Series) -> Dict:\n",
    "    \"\"\"Combine all correlation analyses into comprehensive results.\n",
    "\n",
    "    Args:\n",
    "        series1: First time series data\n",
    "        series2: Second time series data\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing all correlation analyses:\n",
    "        {basic_correlations, rolling_correlation, ccf, delayed_correlations}\n",
    "    \"\"\"\n",
    "    basic_results = calculate_basic_correlations(series1, series2)\n",
    "    rolling_results = calculate_rolling_correlation(series1, series2)\n",
    "    ccf_results = calculate_ccf(series1, series2)\n",
    "    delayed_results = calculate_time_delayed_correlations(series1, series2)\n",
    "    \n",
    "    return {\n",
    "        'basic_correlations': basic_results,\n",
    "        'rolling_correlation': rolling_results,\n",
    "        'ccf': ccf_results,\n",
    "        'delayed_correlations': delayed_results\n",
    "    }\n",
    "\n",
    "# Analysis execution and results compilation\n",
    "print(\"Testing advanced correlation methods across all series pairs...\")\n",
    "\n",
    "summary_results = []\n",
    "\n",
    "for pair in valid_pairs:\n",
    "    series1 = df[pair[0]]\n",
    "    series2 = df[pair[1]]\n",
    "    \n",
    "    results = combine_correlation_analyses(series1, series2)\n",
    "    \n",
    "    summary = {\n",
    "        'Series 1': pair[0],\n",
    "        'Series 2': pair[1],\n",
    "        'CCF_Max_Corr': results['ccf']['ccf']['correlation'],\n",
    "        'CCF_Optimal_Lag': results['ccf']['ccf']['optimal_lag'],\n",
    "        'CCF_Zero_Lag': results['ccf']['ccf']['zero_lag_correlation'],\n",
    "        'Best_Delayed_Lag': max(\n",
    "            results['delayed_correlations']['delayed_correlations'].items(),\n",
    "            key=lambda x: abs(x[1]['pearson']['correlation'])\n",
    "        )[0]\n",
    "    }\n",
    "    summary_results.append(summary)\n",
    "\n",
    "# Results analysis\n",
    "results_df = pd.DataFrame(summary_results)\n",
    "results_df = results_df.iloc[results_df['CCF_Max_Corr'].abs().argsort()[::-1]].head(5)\n",
    "\n",
    "print(\"\\nAdvanced Correlation Analysis Results:\")\n",
    "print(results_df)\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"Average Optimal Lag: {results_df['CCF_Optimal_Lag'].mean():.2f}\")\n",
    "print(f\"Max CCF Correlation: {results_df['CCF_Max_Corr'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cfaabc-33e2-4d44-bc27-30d12dc1e035",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Step 4 of 6: Analysis Framework and Method Comparison\n",
    "\n",
    "- Dependencies: Cell 1 (environment setup)\n",
    "- Outputs: Structured comparison framework and method evaluation\n",
    "\n",
    "### Description:\n",
    "- Compares correlation methods\n",
    "- Determines optimal methods per relationship\n",
    "- Conducts significance analysis\n",
    "- Generates sorted relationship rankings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff90ba5-5458-4224-8e98-fa0864c885a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 4 of 6: Analysis Framework and Method Comparison\n",
    "\n",
    "\"\"\"TRACES Analysis Framework.\n",
    "\n",
    "Implements relationship classification and method comparison logic for time series pairs,\n",
    "providing automated relationship type detection and confidence scoring.\n",
    "\"\"\"\n",
    "\n",
    "def analyze_relationship_type(results: Dict) -> Dict:\n",
    "    \"\"\"Classify relationship type between time series variables.\n",
    "\n",
    "    Args:\n",
    "        results: Combined correlation results containing:\n",
    "                basic_correlations, ccf, rolling_correlation, delayed_correlations\n",
    "\n",
    "    Returns:\n",
    "        Classification results dictionary:\n",
    "        {primary_type, confidence, supporting_metrics, method_recommendations}\n",
    "    \"\"\"\n",
    "    basic = results['basic_correlations']\n",
    "    ccf = results['ccf']\n",
    "    rolling = results['rolling_correlation']\n",
    "    delayed = results['delayed_correlations']\n",
    "    \n",
    "    pearson_spearman_diff = abs(basic['pearson']['correlation'] - \n",
    "                               basic['spearman']['correlation'])\n",
    "    rolling_std = rolling['rolling_correlation']['std']\n",
    "    lag_impact = ccf['ccf']['lag_strength_ratio']\n",
    "    \n",
    "    classification = {\n",
    "        'primary_type': None,\n",
    "        'confidence': 0.0,\n",
    "        'supporting_metrics': {},\n",
    "        'method_recommendations': []\n",
    "    }\n",
    "    \n",
    "    if pearson_spearman_diff < 0.1 and rolling_std < 0.2:\n",
    "        classification['primary_type'] = 'linear'\n",
    "        classification['method_recommendations'].append('pearson')\n",
    "    elif pearson_spearman_diff > 0.2:\n",
    "        classification['primary_type'] = 'non_linear'\n",
    "        classification['method_recommendations'].extend(['spearman', 'kendall'])\n",
    "    elif lag_impact > 1.2:\n",
    "        classification['primary_type'] = 'lagged'\n",
    "        classification['method_recommendations'].append('ccf')\n",
    "    else:\n",
    "        classification['primary_type'] = 'complex'\n",
    "        classification['method_recommendations'].extend(['ccf', 'spearman'])\n",
    "    \n",
    "    classification['confidence'] = calculate_confidence(results)\n",
    "    \n",
    "    return classification\n",
    "\n",
    "def calculate_confidence(results: Dict) -> float:\n",
    "    \"\"\"Calculate confidence score for relationship classification.\n",
    "\n",
    "    Args:\n",
    "        results: Combined correlation results\n",
    "\n",
    "    Returns:\n",
    "        Confidence score (0-1) based on significance tests and correlation strengths\n",
    "    \"\"\"\n",
    "    basic = results['basic_correlations']\n",
    "    significant_count = sum([1 for method in basic.values() if method['significant']])\n",
    "    \n",
    "    confidence = (significant_count / 3) * \\\n",
    "                 max(abs(basic['pearson']['correlation']),\n",
    "                     abs(basic['spearman']['correlation']),\n",
    "                     abs(basic['kendall']['correlation']))\n",
    "    \n",
    "    return round(confidence, 3)\n",
    "\n",
    "def create_summary_table(series1_name: str, series2_name: str, \n",
    "                        results: Dict, classification: Dict) -> pd.DataFrame:\n",
    "    \"\"\"Create comprehensive summary of correlation analyses.\n",
    "\n",
    "    Args:\n",
    "        series1_name: Name of first time series\n",
    "        series2_name: Name of second time series\n",
    "        results: Combined correlation results\n",
    "        classification: Relationship classification results\n",
    "\n",
    "    Returns:\n",
    "        DataFrame containing correlation analysis summary\n",
    "    \"\"\"\n",
    "    summary = {\n",
    "        'Series 1': series1_name,\n",
    "        'Series 2': series2_name,\n",
    "        'Relationship Type': classification['primary_type'],\n",
    "        'Confidence': classification['confidence'],\n",
    "        'Best Method': ', '.join(classification['method_recommendations']),\n",
    "        'Pearson': results['basic_correlations']['pearson']['correlation'],\n",
    "        'Spearman': results['basic_correlations']['spearman']['correlation'],\n",
    "        'Kendall': results['basic_correlations']['kendall']['correlation'],\n",
    "        'Max CCF': results['ccf']['ccf']['correlation'],\n",
    "        'Optimal Lag': results['ccf']['ccf']['optimal_lag'],\n",
    "        'Rolling Mean': results['rolling_correlation']['rolling_correlation']['mean']\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame([summary])\n",
    "\n",
    "# Analysis execution and results compilation\n",
    "print(\"Testing analysis framework across all series pairs...\")\n",
    "\n",
    "all_summaries = []\n",
    "\n",
    "for pair in valid_pairs:\n",
    "    series1 = df[pair[0]]\n",
    "    series2 = df[pair[1]]\n",
    "    \n",
    "    results = combine_correlation_analyses(series1, series2)\n",
    "    classification = analyze_relationship_type(results)\n",
    "    \n",
    "    summary = create_summary_table(pair[0], pair[1], results, classification)\n",
    "    all_summaries.append(summary)\n",
    "\n",
    "# Results analysis\n",
    "full_results = pd.concat(all_summaries, ignore_index=True)\n",
    "full_results['max_correlation'] = full_results[['Pearson', 'Spearman', 'Kendall']].abs().max(axis=1)\n",
    "full_results = full_results.nlargest(24, 'max_correlation')\n",
    "full_results = full_results.drop('max_correlation', axis=1)\n",
    "\n",
    "print(\"\\nRelationship Analysis Results:\")\n",
    "print(full_results.to_string())\n",
    "\n",
    "print(\"\\nRelationship Type Distribution:\")\n",
    "print(full_results['Relationship Type'].value_counts())\n",
    "\n",
    "print(\"\\nConfidence Statistics:\")\n",
    "print(f\"Mean Confidence: {full_results['Confidence'].mean():.3f}\")\n",
    "print(f\"Max Confidence: {full_results['Confidence'].max():.3f}\")\n",
    "\n",
    "print(\"\\nRecommended Methods Distribution:\")\n",
    "full_results['Best Method'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86ff7ed-5c48-4e7e-a66a-a1c34590ff12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Step 5 of 6: Visualization Functions\n",
    "\n",
    "- Dependencies: Cells 1 & 4 (environment and correlation results)\n",
    "- Outputs: Comprehensive visualization suite\n",
    "\n",
    "### Description:\n",
    "\n",
    "- Generates correlation comparisons\n",
    "- Creates relationship matrix (top $N$ pairs)\n",
    "  - Optimized for even-numbered top relationships\n",
    "  - Includes strategic NaN visualization\n",
    "- Displays method performance analysis\n",
    "- Shows CCF patterns\n",
    "- Supports analytical decision-making\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe885bff-0f36-4db3-b942-2313087188c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 5 of 6: Visualization Functions\n",
    "\n",
    "\"\"\"TRACES Visualization Suite.\n",
    "\n",
    "Implements comprehensive visualization functions for time series relationship analysis,\n",
    "including correlation comparisons, relationship matrices, and CCF patterns.\n",
    "\"\"\"\n",
    "\n",
    "def plot_correlation_comparison(full_results: pd.DataFrame, figsize=(15, 10)) -> None:\n",
    "    \"\"\"Plot comparative visualization of correlation methods.\n",
    "\n",
    "    Generates grouped bar plot comparing Pearson, Spearman, and Kendall\n",
    "    correlations for top relationships.\n",
    "\n",
    "    Args:\n",
    "        full_results: DataFrame containing analysis results\n",
    "        figsize: Figure dimensions (width, height)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    plot_data = full_results[['Series 1', 'Series 2', 'Pearson', 'Spearman', 'Kendall']].copy()\n",
    "    plot_data.loc[:, 'Pair'] = plot_data['Series 1'] + ' - ' + plot_data['Series 2']\n",
    "    \n",
    "    plot_data_melted = pd.melt(\n",
    "        plot_data,\n",
    "        id_vars=['Pair'],\n",
    "        value_vars=['Pearson', 'Spearman', 'Kendall'],\n",
    "        var_name='Method',\n",
    "        value_name='Correlation'\n",
    "    )\n",
    "    \n",
    "    ax = sns.barplot(\n",
    "        data=plot_data_melted,\n",
    "        x='Pair',\n",
    "        y='Correlation',\n",
    "        hue='Method',\n",
    "        palette='coolwarm'\n",
    "    )\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Comparison of Correlation Methods Across Top Relationships')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_relationship_matrix(full_results: pd.DataFrame, figsize=(12, 8)) -> None:\n",
    "    \"\"\"Plot matrix visualization of relationship types and confidence scores.\n",
    "\n",
    "    Note: This visualization is designed for the top N strongest relationships \n",
    "    (where N is an even number, default from previous is 24, but customizable).\n",
    "    The matrix will naturally contain blank/NaN cells due to the cartesian product of\n",
    "    unique series pairs, which is expected behavior amidst the other strongest relationship patterns.\n",
    "\n",
    "    Args:\n",
    "        full_results: DataFrame containing top N analysis results (N should be even)\n",
    "        figsize: Figure dimensions (width, height)\n",
    "    \"\"\"\n",
    "    sorted_results = full_results.sort_values('Confidence', ascending=False)\n",
    "    pairs = list(zip(sorted_results['Series 1'], sorted_results['Series 2']))\n",
    "    \n",
    "    series1_ordered = []\n",
    "    series2_ordered = []\n",
    "    for s1, s2 in pairs:\n",
    "        if s1 not in series1_ordered:\n",
    "            series1_ordered.append(s1)\n",
    "        if s2 not in series2_ordered:\n",
    "            series2_ordered.append(s2)\n",
    "    \n",
    "    matrix_data = pd.DataFrame(np.nan, \n",
    "                             index=series1_ordered,\n",
    "                             columns=series2_ordered)\n",
    "    \n",
    "    type_matrix = pd.DataFrame('',\n",
    "                             index=series1_ordered,\n",
    "                             columns=series2_ordered)\n",
    "    \n",
    "    for _, row in sorted_results.iterrows():\n",
    "        matrix_data.loc[row['Series 1'], row['Series 2']] = row['Confidence']\n",
    "        type_matrix.loc[row['Series 1'], row['Series 2']] = row['Relationship Type']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    mask = np.isnan(matrix_data)\n",
    "    \n",
    "    sns.heatmap(\n",
    "        matrix_data,\n",
    "        annot=type_matrix,\n",
    "        fmt='',\n",
    "        ax=ax,\n",
    "        cmap='coolwarm',\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        mask=mask,\n",
    "        cbar_kws={'label': 'Confidence Score'}\n",
    "    )\n",
    "    \n",
    "    plt.title('Top Relationships by Confidence Score')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_method_performance(full_results: pd.DataFrame, figsize=(15, 6)) -> None:\n",
    "    \"\"\"Plot method performance across relationship types.\n",
    "\n",
    "    Args:\n",
    "        full_results: DataFrame containing analysis results\n",
    "        figsize: Figure dimensions (width, height)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    method_counts = pd.DataFrame(full_results.groupby('Relationship Type')['Best Method'].value_counts())\n",
    "    method_counts = method_counts.unstack(fill_value=0)\n",
    "    \n",
    "    colors = plt.cm.coolwarm(np.linspace(0, 1, len(method_counts.columns)))\n",
    "    method_counts.plot(\n",
    "        kind='bar',\n",
    "        stacked=True,\n",
    "        ax=ax,\n",
    "        color=colors\n",
    "    )\n",
    "    \n",
    "    plt.title('Method Performance by Relationship Type')\n",
    "    plt.xlabel('Relationship Type')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Best Method', bbox_to_anchor=(1.05, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_ccf_analysis(full_results: pd.DataFrame, figsize=(12, 6)) -> None:\n",
    "    \"\"\"Plot CCF analysis showing correlation strength vs lag patterns.\n",
    "\n",
    "    Args:\n",
    "        full_results: DataFrame containing analysis results\n",
    "        figsize: Figure dimensions (width, height)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    scatter = plt.scatter(\n",
    "        full_results['Optimal Lag'],\n",
    "        full_results['Max CCF'].abs(),\n",
    "        c=full_results['Confidence'],\n",
    "        cmap='coolwarm',\n",
    "        s=100\n",
    "    )\n",
    "    \n",
    "    for idx, row in full_results.iterrows():\n",
    "        plt.annotate(\n",
    "            row['Relationship Type'],\n",
    "            (row['Optimal Lag'], abs(row['Max CCF'])),\n",
    "            xytext=(5, 5),\n",
    "            textcoords='offset points',\n",
    "            fontsize=8\n",
    "        )\n",
    "    \n",
    "    plt.colorbar(scatter, label='Confidence Score')\n",
    "    plt.title('CCF Analysis: Maximum Correlation vs Optimal Lag')\n",
    "    plt.xlabel('Optimal Lag')\n",
    "    plt.ylabel('|Maximum CCF|')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualization generation\n",
    "print(\"Generating visualization suite...\")\n",
    "\n",
    "plot_correlation_comparison(full_results)\n",
    "plot_relationship_matrix(full_results)\n",
    "plot_method_performance(full_results)\n",
    "plot_ccf_analysis(full_results)\n",
    "\n",
    "print(\"\\nVisualization suite complete. Plot descriptions:\")\n",
    "print(\"1. Bar plot: Correlation method comparison across relationships\")\n",
    "print(\"2. Matrix: Relationship types and confidence scores\")\n",
    "print(\"3. Bar chart: Method effectiveness by relationship type\")\n",
    "print(\"4. Scatter plot: CCF patterns and lag relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7886ff0-48f6-424c-bd82-aa89f7a476b2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "# Step 6 of 6: Results Processing and Full Dataset Analysis\n",
    "\n",
    "- Dependencies: Cell 1 (environment setup)\n",
    "- Outputs: Complete analysis across all valid pairs\n",
    "\n",
    "### Description:\n",
    "\n",
    "- Processes all valid relationship pairs\n",
    "- Groups by relationship types\n",
    "- Orders by correlation strength\n",
    "- Generates comprehensive statistics\n",
    "- Provides detailed analytical summaries\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba7a2e1-12f3-430a-8742-b715e3d7780c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 6 of 6: Results Processing and Full Dataset Analysis\n",
    "\n",
    "\"\"\"TRACES Results Processing Module.\n",
    "\n",
    "Implements comprehensive dataset analysis pipeline, including correlation processing,\n",
    "summary statistics generation, and detailed results reporting by relationship type.\n",
    "\"\"\"\n",
    "\n",
    "def analyze_full_dataset(df: pd.DataFrame, \n",
    "                        valid_pairs: List[Tuple[str, str]]) -> pd.DataFrame:\n",
    "    \"\"\"Process all valid series pairs through correlation analysis pipeline.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing time series data\n",
    "        valid_pairs: List of valid series pairs for analysis\n",
    "\n",
    "    Returns:\n",
    "        DataFrame containing comprehensive analysis results\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    \n",
    "    for pair in valid_pairs:\n",
    "        series1 = df[pair[0]]\n",
    "        series2 = df[pair[1]]\n",
    "        \n",
    "        results = combine_correlation_analyses(series1, series2)\n",
    "        classification = analyze_relationship_type(results)\n",
    "        \n",
    "        summary = {\n",
    "            'Series 1': pair[0],\n",
    "            'Series 2': pair[1],\n",
    "            'Relationship Type': classification['primary_type'],\n",
    "            'Confidence': classification['confidence'],\n",
    "            'Best Method': ', '.join(classification['method_recommendations']),\n",
    "            'Pearson': results['basic_correlations']['pearson']['correlation'],\n",
    "            'Spearman': results['basic_correlations']['spearman']['correlation'],\n",
    "            'Kendall': results['basic_correlations']['kendall']['correlation'],\n",
    "            'Max CCF': results['ccf']['ccf']['correlation'],\n",
    "            'Optimal Lag': results['ccf']['ccf']['optimal_lag'],\n",
    "            'Rolling Mean': results['rolling_correlation']['rolling_correlation']['mean'],\n",
    "            'Abs_Max_Corr': max(abs(results['basic_correlations']['pearson']['correlation']),\n",
    "                               abs(results['basic_correlations']['spearman']['correlation']),\n",
    "                               abs(results['basic_correlations']['kendall']['correlation']))\n",
    "        }\n",
    "        results_list.append(summary)\n",
    "    \n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    return results_df.sort_values('Abs_Max_Corr', ascending=False)\n",
    "\n",
    "def generate_summary_statistics(results_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"Generate comprehensive summary statistics from analysis results.\n",
    "\n",
    "    Args:\n",
    "        results_df: DataFrame containing analysis results\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of summary statistics including relationship types,\n",
    "        confidence scores, and correlation strength distributions\n",
    "    \"\"\"\n",
    "    relationship_types = results_df['Relationship Type'].value_counts().to_dict()\n",
    "    avg_confidence = results_df['Confidence'].mean()\n",
    "    \n",
    "    method_counts = {}\n",
    "    for methods in results_df['Best Method']:\n",
    "        for method in methods.split(', '):\n",
    "            method_counts[method] = method_counts.get(method, 0) + 1\n",
    "    \n",
    "    strong_correlations = len(results_df[results_df['Abs_Max_Corr'] > 0.7])\n",
    "    moderate_correlations = len(results_df[\n",
    "        (results_df['Abs_Max_Corr'] >= 0.3) & \n",
    "        (results_df['Abs_Max_Corr'] <= 0.7)\n",
    "    ])\n",
    "    weak_correlations = len(results_df[results_df['Abs_Max_Corr'] < 0.3])\n",
    "    \n",
    "    return {\n",
    "        'relationship_types': relationship_types,\n",
    "        'avg_confidence': avg_confidence,\n",
    "        'method_counts': method_counts,\n",
    "        'strong_correlations': strong_correlations,\n",
    "        'moderate_correlations': moderate_correlations,\n",
    "        'weak_correlations': weak_correlations\n",
    "    }\n",
    "\n",
    "def print_grouped_results(results_df: pd.DataFrame) -> None:\n",
    "    \"\"\"Print detailed analysis results grouped by relationship type.\n",
    "\n",
    "    Args:\n",
    "        results_df: DataFrame containing analysis results\n",
    "    \"\"\"\n",
    "    grouped = results_df.groupby('Relationship Type')\n",
    "    \n",
    "    for rel_type, group in grouped:\n",
    "        print(f\"\\n=== {rel_type.upper()} RELATIONSHIPS ===\")\n",
    "        print(f\"Number of pairs: {len(group)}\")\n",
    "        \n",
    "        top_pairs = group.nlargest(5, 'Abs_Max_Corr')\n",
    "        \n",
    "        print(\"\\nTop 5 strongest correlations:\")\n",
    "        for _, row in top_pairs.iterrows():\n",
    "            print(f\"\\n{row['Series 1']} vs {row['Series 2']}:\")\n",
    "            print(f\"  Absolute Max Correlation: {row['Abs_Max_Corr']:.3f}\")\n",
    "            print(f\"  Best Method(s): {row['Best Method']}\")\n",
    "            print(f\"  Confidence: {row['Confidence']:.3f}\")\n",
    "            if abs(row['Optimal Lag']) > 0:\n",
    "                print(f\"  Optimal Lag: {row['Optimal Lag']}\")\n",
    "        \n",
    "        print(f\"\\nGroup Statistics:\")\n",
    "        print(f\"  Mean Confidence: {group['Confidence'].mean():.3f}\")\n",
    "        print(f\"  Mean Abs Correlation: {group['Abs_Max_Corr'].mean():.3f}\")\n",
    "        print(f\"  Most Common Best Method: {group['Best Method'].mode().iloc[0]}\")\n",
    "\n",
    "def run_full_analysis(df: pd.DataFrame, \n",
    "                     valid_pairs: List[Tuple[str, str]]) -> pd.DataFrame:\n",
    "    \"\"\"Execute complete TRACES analysis pipeline on dataset.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing time series data\n",
    "        valid_pairs: List of valid series pairs for analysis\n",
    "\n",
    "    Returns:\n",
    "        DataFrame containing complete analysis results\n",
    "    \"\"\"\n",
    "    print(\"\\nInitiating TRACES Correlation Analysis...\")\n",
    "    \n",
    "    try:\n",
    "        full_results = analyze_full_dataset(df, valid_pairs)\n",
    "        print(f\"Completed analysis of {len(full_results)} pairs\")\n",
    "        \n",
    "        print(\"\\nGenerating summary statistics...\")\n",
    "        summary_stats = generate_summary_statistics(full_results)\n",
    "        \n",
    "        print(\"\\nANALYSIS SUMMARY:\")\n",
    "        print(f\"Total pairs analyzed: {len(full_results)}\")\n",
    "        print(\"\\nRelationship Types Distribution:\")\n",
    "        for rel_type, count in summary_stats['relationship_types'].items():\n",
    "            print(f\"{rel_type}: {count}\")\n",
    "        \n",
    "        print(\"\\nAverage Confidence Score:\", \n",
    "              f\"{summary_stats['avg_confidence']:.3f}\")\n",
    "        \n",
    "        print(\"\\nCorrelation Strength Distribution:\")\n",
    "        print(f\"Strong correlations (>0.7): {summary_stats['strong_correlations']}\")\n",
    "        print(f\"Moderate correlations (0.3-0.7): \"\n",
    "              f\"{summary_stats['moderate_correlations']}\")\n",
    "        print(f\"Weak correlations (<0.3): {summary_stats['weak_correlations']}\")\n",
    "        \n",
    "        print(\"\\nPrinting detailed results by group...\")\n",
    "        print_grouped_results(full_results)\n",
    "        \n",
    "        return full_results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during analysis: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Execute full analysis pipeline\n",
    "final_results = run_full_analysis(df, valid_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a41e01-6c72-418b-895e-0d986c3953bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
